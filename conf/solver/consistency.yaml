# @package _global_
defaults:
  - _self_
  - /prior: gauss_truncate
  - /sde: vp_10
  - /model@generative_ctrl: lerp  # lerp for DIS, score for DDS
  # CM
  - /model@consistency_model: consistency_model
  # - /loss: consistency
  - /lr_scheduler: step
  - /utils@scheduler: scheduler
  - /utils@grad_clip: grad_clip
  - /utils@ema: ema

# Solver
solver:
  _target_: sde_sampler.solver.consistency.ConsistencySolver
loss:
  _target_: sde_sampler.losses.consistency.ConsistencyDistillation

# Pretrained Model
generative_ctrl:
  clip_score: 10.
  clip_model: 10.
  project: "teacher"  # Overwrite this with project name
  pretrained_path: "logs/${generative_ctrl.project}/ckpt/ckpt_final.pt"

# Model
consistency_model:
  clip_model: 10.
  sigma_data: 0.5

batch_t: True

# Train
train_steps: 150000
train_batch_size: 512
train_timesteps:
  _target_: sde_sampler.utils.common.get_timesteps
  _partial_: True
  start: 0.0
  end: ${sde.terminal_t}
  steps: 128
cm_train_timesteps: 18
cm_eval_timesteps: 3  # single-step (includes t_0 and t_N)
max_loss:
max_grad:
scale_loss: ${eval:1/${target.dim}}
clip_target:

# EMA, optimizer, scheduler
optim:
  _target_: torch.optim.Adam
  lr: 0.005 
  weight_decay: 1e-7
grad_clip:
  _target_: torch.nn.utils.clip_grad_norm_
  _partial_: True
  max_norm: 1.
  norm_type: 2.0
  error_if_nonfinite: False
ema_device:

# Eval and checkpointing
eval_timesteps: ${train_timesteps}
eval_batch_size: 6000
eval_stddev_steps:
eval_interval: 500
eval_device:
eval_init: True
ckpt_interval: 5000
log_interval: 50

