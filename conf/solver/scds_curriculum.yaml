# @package _global_
defaults:
  - _self_
  - /prior: gauss_truncate
  - /sde: vp_10
  - /model@generative_ctrl: lerp_step
  - /lr_scheduler: step
  - /utils@scheduler: scheduler
  - /utils@grad_clip: grad_clip
  - /utils@ema: ema

generative_ctrl:
  clip_score: 10.
  clip_model: 10.

solver:
  _target_: sde_sampler.solver.oc.SCDS

# Time Schedules 
train_timesteps:
  _target_: sde_sampler.utils.common.get_timesteps
  _partial_: True
  start: 0.0
  end: ${sde.terminal_t}
  steps: 128
eval_timesteps:
  _target_: sde_sampler.utils.common.get_timesteps
  _partial_: True
  start: 0.0
  end: ${sde.terminal_t}
  steps: 128
curriculum:
  method_N: root
  method_d: biaised
  min_steps: 16

# Training, loss, and Optim
train_steps: 20000
train_batch_size: 512  # 2048
max_loss:
max_grad:
scale_loss: ${eval:1/${target.dim}}
clip_target:
optim:
  _target_: torch.optim.Adam
  lr: 0.005
  weight_decay: 1e-7
ema_device:

# Eval and checkpointing
eval_batch_size: 6000
eval_stddev_steps:
eval_interval: 500
eval_device:
eval_init: True
ckpt_interval: 5000
log_interval: 50

